[{"authors":null,"categories":null,"content":"हृषीकेश राजेश तेरदाळकर  Hrishikesh is a research scholar at the Department of Computer Science and Engineering, IIT Kanpur working under the supervision of Prof. Arnab Bhattacharya.\nHis current research interests include Natural Language Processing, Computational Linguistics, Information Retrieval, Data Mining and Artificial Intelligence. He is particularly interested in Indian languages. He can read, write and speak Marathi, Sanskrit, Hindi and English. He likes building user-friendly GUIs and CLIs for various purposes.\n Download my resumé. -- ","date":1673222400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1673222400,"objectID":"9e370bd7161e19b2ff5ea464c359c815","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"हृषीकेश राजेश तेरदाळकर  Hrishikesh is a research scholar at the Department of Computer Science and Engineering, IIT Kanpur working under the supervision of Prof. Arnab Bhattacharya.\nHis current research interests include Natural Language Processing, Computational Linguistics, Information Retrieval, Data Mining and Artificial Intelligence.","tags":null,"title":"Hrishikesh Terdalkar","type":"authors"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"965dec720687c23fd364e91333863e74","permalink":"/publication/wsc2023_2/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/publication/wsc2023_2/","section":"publication","summary":"We present Chandojñānam, a web-based Sanskrit meter (Chanda) identification and utilization system. In addition to the core functionality of identifying meters, it sports a friendly user interface to display the scansion, which is a graphical representation of the metrical pattern. The system supports identification of meters from uploaded images by using optical character recognition (OCR) engines in the backend. It is also able to process entire text files at a time. The text can be processed in two modes, either by treating it as a list of individual lines, or as a collection of verses. When a line or a verse does not correspond exactly to a known meter, Chandojñānam is capable of finding fuzzy (i.e., approximate and close) matches based on sequence matching. This opens up the scope of a meter based correction of erroneous digital corpora. The system is available for use at https://sanskrit.iitk.ac.in/jnanasangraha/chanda/, and the source code in the form of a Python library is made available at https://github.com/hrishikeshrt/chanda/.","tags":["sanskrit","prosody","meter identification","software"],"title":"Chandojñānam: A Sanskrit Meter Identification and Utilization System","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"22daa85821ec7dcf167ee65026ea63b8","permalink":"/publication/wsc2023demo_1/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_1/","section":"publication","summary":"Jñānasaṅgrahaḥ is a web-based collection of several computational applications related to the Sanskrit language. The aim is to highlight the features of Sanskrit language in a way that is approachable for an enthusiastic user, even if she has a limited Sanskrit background.","tags":["sanskrit","software","jnanasangraha","demonstration"],"title":"Jñānasaṅgrahaḥ: A Collection of Computational Applications related to Sanskrit","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"830f86ad6ed0cdfdf19cf8f655f85234","permalink":"/publication/wsc2023demo_3/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_3/","section":"publication","summary":"PyCDSL is a Python library that provides programmer friendly interface to Cologne Digital Sanskrit Dictionaries (CDSD). The library serves as a corpus management tool to download, update and access dictionaries from CDSD. The tool provides a command line interface for ease of search and a programmable interface for using CDSD in computational linguistic projects written in Python 3.","tags":["sanskrit","software","library","dictionary","demonstration"],"title":"PyCDSL: A Programmatic Interface to Cologne Digital Sanskrit Dictionaries","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya","Madhulika Dubey","Ramamurthy S","Bhavna Naneria Singh"],"categories":null,"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"551adc5ffa8aac4dce57ee070323c60a","permalink":"/publication/wsc2023_1/","publishdate":"2022-01-12T00:00:00Z","relpermalink":"/publication/wsc2023_1/","section":"publication","summary":"Knowledge bases (KB) are an important resource in a number of natural language processing (NLP) and information retrieval (IR) tasks, such as semantic search, automated question-answering etc. They are also useful for researchers trying to gain information from a text. Unfortunately, however, the state-of-the-art in Sanskrit NLP does not yet allow automated construction of knowledge bases due to unavailability or lack of sufficient accuracy of tools and methods. Thus, in this work, we describe our efforts on manual annotation of Sanskrit text for the purpose of knowledge graph (KG) creation. We choose the chapter Dhānyavarga from Bhāvaprakāśanighaṇṭu of the Ayurvedic text Bhāvaprakāśa for annotation. The constructed knowledge graph contains 410 entities and 764 relationships. Since Bhāvaprakāśanighaṇṭu is a technical glossary text that describes various properties of different substances, we develop an elaborate ontology to capture the semantics of the entity and relationship types present in the text. To query the knowledge graph, we design 31 query templates that cover most of the common question patterns. For both manual annotation and querying, we customize the Sangrahaka framework previously developed by us. The entire system including the dataset is available from https://sanskrit.iitk.ac.in/ayurveda. We hope that the knowledge graph that we have created through manual annotation and subsequent curation will help in development and testing of NLP tools in future as well as studying of the Bhāvaprakāśanighaṇṭu text.","tags":["sanskrit","ayurveda","annotation","knowledge graph"],"title":"Semantic Annotation and Querying Framework based on Semi-structured Ayurvedic Text","type":"publication"},{"authors":["Hrishikesh Terdalkar","Mahesh A V S D S","Shubhangi Agarwal","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"773ed3c8591d86213296305109bfbb67","permalink":"/publication/wsc2023demo_2/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_2/","section":"publication","summary":"Vaiyyākaraṇaḥ is a Telegram bot aimed towards helping the learners of Sanskrit grammar (vyākaraṇa). The salient features of Vaiyyākaraṇaḥ are: stem finder (prātipadikam), declension generation (subantāḥ), root finder (dhātuḥ), conjugation generation (tiṅantāḥ) and word segmentation (sandhisamāsau). State-of-the-art datasets, tools and technologies are used to offer these capabilities.","tags":["sanskrit","software","grammar","demonstration"],"title":"Vaiyyākaraṇaḥ: A Sanskrit Grammar Bot for Telegram","type":"publication"},{"authors":["Jivnesh Sandhan","Ashish Gupta","Hrishikesh Terdalkar","Tushar Sandhan","Suvendu Samanta","Laxmidhar Behera","Pawan Goyal"],"categories":null,"content":"","date":1665532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665532800,"objectID":"4db8b4ea56bc4195c89b37f929be7df4","permalink":"/publication/coling2022/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/publication/coling2022/","section":"publication","summary":"The phenomenon of compounding is ubiquitous in Sanskrit. It serves for achieving brevity in expressing thoughts, while simultaneously enriching the lexical and structural formation of the language. In this work, we focus on the Sanskrit Compound Type Identification (SaCTI) task, where we consider the problem of identifying semantic relations between the components of a compound word. Earlier approaches solely rely on the lexical information obtained from the components and ignore the most crucial contextual and syntactic information useful for SaCTI. However, the SaCTI task is challenging primarily due to the implicitly encoded context-sensitive semantic relation between the compound components. Thus, we propose a novel multi-task learning architecture which incorporates the contextual information and enriches the complementary syntactic information using morphological tagging and dependency parsing as two auxiliary tasks. Experiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and 7.7 points (F1-score) absolute gain compared to the state-of-the-art system. Further, our multi-lingual experiments demonstrate the efficacy of the proposed architecture in English and Marathi languages.","tags":["sanskrit","multi-task learning","compound identification","annotation"],"title":"A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in Sanskrit","type":"publication"},{"authors":["Hrishikesh Terdalkar"],"categories":[],"content":"Majority of the Sanskrit literature is in the form of poetry that adheres to the rules of Sanskrit prosody or Chandaḥśāstra, which is the study of Sanskrit meters, known as chandas. The purpose of chanda is primarily to add rhythm to the text so that it is easier to memorize. Additionally, it also helps in preserving the correctness to some extent. Chandojñānam is a web-based Sankrit prosodical meter identification and utilization system. In addition to the core functionality of identifying meters from Sanskrit text, it provides fuzzy (i.e., approximate and close) matches for text that does not correspond exactly to a known metrical pattern. This opens up the scope of a meter based correction of erroneous digital corpora. The system supports identification of meters from uploaded images by using optical character recognition (OCR) engines in the backend. It is also able to process entire text files at a time. The text can be processed in two modes, either by treating it as a list of individual lines, or as a collection of verses.\n","date":1651375800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651375800,"objectID":"6cd7d85915092562467a836a54e0ab26","permalink":"/project/chandojnana/","publishdate":"2022-05-01T09:00:00+05:30","relpermalink":"/project/chandojnana/","section":"project","summary":"Chandojñānam is a web-based Sankrit prosodical meter identification and utilization system. In addition to the core functionality of identifying meters from Sanskrit text, it provides fuzzy (i.e., approximate and close) matches for text that does not correspond exactly to a known metrical pattern. This opens up the scope of a meter based correction of erroneous digital corpora. The system supports identification of meters from uploaded images by using optical character recognition (OCR) engines in the backend. It is also able to process entire text files at a time. The text can be processed in two modes, either by treating it as a list of individual lines, or as a collection of verses.","tags":["sanskrit","software","user interface"],"title":"Chandojñānam","type":"project"},{"authors":["Hrishikesh Terdalkar"],"categories":["demo"],"content":"Devanagari is the fourth most widely adopted writing system in the world, primarily used in the Indian subcontinent. The script is being used for more than 120 languages, some of the more notable languages being, Sanskrit, Hindi, Marathi, Pali, Nepali and several variations of these languages.\nDevanagari text can be transliterated in various standard schemes. There exist several input systems based on these transliteration schemes to enable users easily input the text. More often than not, a user has a preference of scheme to type the input in. Similarly, at times, one faces a need to render it in a different scheme in the PDF document.\nIn my case, I prefer using ibus-m17n to type text in Devanagari. While writing articles that contain Devanagari text, I also faced the need to render the text as IAST in the final PDF. One could always learn to input text in another input scheme, but that may get tedious. Similarly, transliterating each word using online systems such as Aksharamukha can also be a tedious task. So, I was looking for a way where I can type in Devanagari, and have it rendered in IAST after PDF compilation. As a solution, I came up with a system consisting of a small set of LaTeX commands to add custom syntax to LaTeX and a python transliteration script (based on indic-transliteration package) to serve as a middle-layer and process the LaTeX file to create a new LaTeX file with proper transliteration.\nLaTeX Compilation System with Transliteration Support There are two primary components to the system,\n LaTeX Synatx Transliteration Script  LaTeX Syntax XeTeX (xelatex) and LuaTeX (lualatex) have good unicode support and can be used to write Devanagari text. In the current example, I mention the setup with XeTeX.\nWe first add the required packages in the preamble of the LaTeX (.tex) file.\n% This assumes your files are encoded as UTF8 \\usepackage[utf8]{inputenc} % Devanagari Related Packages \\usepackage{fontspec, xunicode, xltxtra}  Using fontspec, we can define environments for font families, to write text in specific scripts. To write Devanagari text, one needs to have a Devanagari font available. (It is assumed here that one may need to write both in Devanagari as well as other transliteration schemes.)\nFor more on Devanagari fonts, you may check the fonts section of this document. In this section, it is assumed that Sanskrit 2003 font is installed in the system.\nTo define the environments as mentioned earlier, we add the following lines in the preamble.\n% Define Fonts \\newfontfamily\\textskt[Script=Devanagari]{Sanskrit 2003} \\newfontfamily\\textiast{Noto Serif} % Commands for Devanagari Transliterations \\newcommand{\\skt}[1]{{\\textskt{#1}}} \\newcommand{\\iast}[1]{{\\textiast{#1}}} \\newcommand{\\Iast}[1]{{\\textiast{#1}}} \\newcommand{\\IAST}[1]{{\\textiast{#1}}}  This provides us with four commands. \\skt{} can be used to render Devanagari text. \\iast{}, \\Iast{} and \\IAST{} can be used to render devanagari text in IAST format in lower case, title case and upper case respectively. It should be noted that from the perspective of LaTeX engine, the commands \\iast{}, \\Iast{} and \\IAST{} are identical. They are just different syntactically to aid the python script to perform transliteration and apply appropriate modifications. It should further be noted that we can define new font families and new commands for any of the valid schemes as per the requirement, which can potentially give us additional commands such \\velthuis{}, \\hk{} and so on.\nMinimal Example Equipped with these commands, and some Devanagari text, we have a minimal example as follows, stored in the file minimal.tex,\n\\documentclass[10pt]{article} % This assumes your files are encoded as UTF8 \\usepackage[utf8]{inputenc} % Devanagari Related Packages \\usepackage{fontspec, xunicode, xltxtra} % Define Fonts \\newfontfamily\\textskt[Script=Devanagari]{Sanskrit 2003} \\newfontfamily\\textiast{Noto Serif} % Commands for Devanagari Transliterations \\newcommand{\\skt}[1]{{\\textskt{#1}}} \\newcommand{\\iast}[1]{{\\textiast{#1}}} \\newcommand{\\Iast}[1]{{\\textiast{#1}}} \\newcommand{\\IAST}[1]{{\\textiast{#1}}} \\title{Transliteration of Devanagari Text} \\author{Hrishikesh Terdalkar} \\begin{document} \\maketitle \\skt{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\iast{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\Iast{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\IAST{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\end{document}  Transliteration Script The python script is used to perform transliteration and some clean-up on the LaTeX.\npython3 finalize.py minimal.tex final.tex  This result in the content being transformed in the following way,\n% ... \\skt{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\iast{ko nvasmin sāmprataṃ loke guṇavān kaśca vīryavān|} \\Iast{Ko Nvasmin Sāmprataṃ Loke Guṇavān Kaśca Vīryavān|} \\IAST{KO NVASMIN SĀMPRATAṂ LOKE GUṆAVĀN KAŚCA VĪRYAVĀN|} % ...  We can now proceed to compile the final.tex file.\nxelatex final  This results in the following output,   Anatomy of the Transliteration Script At the core of the transliteration script, there is a function transliterate_between.\ndef transliterate_between( text: str, from_scheme: str, to_scheme: str, start_pattern: str, end_pattern: str, post_hook: Callable[[str], str] = lambda x: x, ) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Transliterate the text appearing between two patterns Only the text appearing between patterns `start_pattern` and `end_pattern` it transliterated. `start_pattern` and `end_pattern` can appear multiple times in the full text, and for every occurrence, the text between them is transliterated. `from_scheme` and `to_scheme` should be compatible with scheme names from `indic-transliteration` Parameters ---------- text : str Full text from_scheme : str Input transliteration scheme to_scheme : str Output transliteration scheme start_pattern : regexp Pattern describing the start tag end_pattern : regexp Pattern describing the end tag post_hook : Callable[[str], str], optional Function to be applied on the text within tags after transliteration The default is `lambda x: x`. Returns ------- str Text after replacements \u0026quot;\u0026quot;\u0026quot; if from_scheme == to_scheme: return text def transliterate_match(matchobj): target = matchobj.group(1) replacement = transliterate(target, from_scheme, to_scheme) replacement = post_hook(replacement) return f\u0026quot;{start_pattern}{replacement}{end_pattern}\u0026quot; pattern = \u0026quot;%s(.*?)%s\u0026quot; % (re.escape(start_pattern), re.escape(end_pattern)) return re.sub(pattern, transliterate_match, text, flags=re.DOTALL)  We can provide the start and end patterns as \\iast{ and } respsectively, to transliterate the text enclosed in these tags.\nUsing this function, we can write a generic function to work with any transliteration scheme.\ndef latex_transliteration( input_text: str, from_scheme: str, to_scheme: str ) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Transliaterate parts of the LaTeX input enclosed in scheme tags A scheme tag is of the form `\\\\to_scheme_lowercase{}` and is used when the desired output is in `to_scheme`. i.e., - Tags for IAST scheme are enclosed in \\\\iast{} tags - Tags for VH scheme are enclosed in \\\\vh{} tags - ... Parameters ---------- input_text : str Input text from_scheme : str Transliteration scheme of the text written within the input tags to_scheme : str Transliteration scheme to which the text within tags should be transliterated Returns ------- str Text after replacement of text within the scheme tags \u0026quot;\u0026quot;\u0026quot; start_tag_pattern = f\u0026quot;\\\\{to_scheme.lower()}\u0026quot; end_tag_pattern = \u0026quot;}\u0026quot; return transliterate_between( input_text, from_scheme=from_scheme, to_scheme=to_scheme, start_pattern=start_tag_pattern, end_pattern=end_tag_pattern )  Note: The names of schemes (and therefore the corresponding LaTeX commands) have to conform to the names of schemes used by the indic-transliteration package.\nIAST is a case-insensitive transliteration scheme, and as such, we might be interested in specific capitalization of certain words (e.g. proper nouns). We can use the post_hook argument to provide this function. Using that, we can create a function to handle the three variants of IAST mentioned previously, namely, \\iast{} (lower), \\Iast{} (title) and \\IAST{} (upper).\ndef devanagari_to_iast(input_text: str) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;Transliaterate parts of the input enclosed in \\\\iast{}, \\\\Iast{} or \\\\IAST{} tags from Devanagari to IAST Text in \\\\Iast{} tags also undergoes a `.title()` post-hook. Text in \\\\IAST{} tags also undergoes a `.upper()` post-hook. Parameters ---------- input_text : str Input text Returns ------- str Text after replacement of text within the IAST tags \u0026quot;\u0026quot;\u0026quot; intermediate_text = transliterate_between( input_text, from_scheme=sanscript.DEVANAGARI, to_scheme=sanscript.IAST, start_pattern=\u0026quot;\\\\iast{\u0026quot;, end_pattern=\u0026quot;}\u0026quot; ) intermediate_text = transliterate_between( intermediate_text, from_scheme=sanscript.DEVANAGARI, to_scheme=sanscript.IAST, start_pattern=\u0026quot;\\\\Iast{\u0026quot;, end_pattern=\u0026quot;}\u0026quot;, post_hook=lambda x: x.title() ) final_text = transliterate_between( intermediate_text, from_scheme=sanscript.DEVANAGARI, to_scheme=sanscript.IAST, start_pattern=\u0026quot;\\\\IAST{\u0026quot;, end_pattern=\u0026quot;}\u0026quot;, post_hook=lambda x: x.upper() ) return final_text  Finally, there are other utility functions to remove comments and clean excessive whitespaces.\nExtras Additionally, we may want some more structure to our setup, such as,\n Separation of ontent into multiple files  \\input{sections/section_devanagari.tex} \\input{sections/section_iast_lower.tex} \\input{sections/section_iast_title.tex} \\input{sections/section_iast_upper.tex}   Bibliography  \\bibliographystyle{acm} \\bibliography{papers}  Final LaTeX Preparation We may have used the scheme tags across multiple sections. One option is to apply the transliteration script on every section file, to create a new set of section files and use those to compile the final LaTeX file.\nA simpler solution is available in the form of latexpand which resolves the \\input{} commands to actually include the content and create a single consolidated LaTeX file.\nlatexpand main.tex \u0026gt; single.tex  Now, we can run the python script on this file to resolve the transliteration tags.\npython3 finalize.py main.tex final.tex  Compilation When working with BibTeX, we often need to multiple times to get the correct rendering of references in the PDF. Usually, this requires\nxelatex final bibtex final xelatex final xelatex final  Alternatively, we can use latexmk which takes care of the tedious compilation routines and reduces our job to a single command,\nlatexmk -pdflatex='xelatex %O %S' -pdf -ps- -dvi- final.tex  Another benefit of using latexmk is, we can clean the numerous files generated by LaTeX engine using a one-liner as well,\nlatexmk -c  Makefile Finally, we can place all of the console commands together in a Makefile.\nall: .all .all: main.tex sections/*.tex papers.bib latexpand main.tex \u0026gt; single.tex python3 finalize.py single.tex final.tex latexmk -pdflatex='xelatex %O %S' -pdf -ps- -dvi- final.tex clear: latexmk -C rm single.tex rm final.tex clean: latexmk -c  Thus, now we can focus on writing content in the .tex files and once we are done, simply use the command,\nmake  Requirements We have made use of a number of external tools, and it is required to have these setup prior to the described solution.\nMinimal Requirements The minimal example mentioned earlier requires only three things,\n XeLaTeX (unicode support) (included in TeX Live) Python3 indic-transliteration  Extra Requirements The extras have some more dependencies.\n BibTeX (optional) (bibliography support) latexpand (optional) (resolve \\input{}) latexmk (optional) (simpler TeX compilation)  Devanagari Fonts Nowadays, there are several good Devanagari fonts available. Google Fonts also provides a wide variety of Devanagari fonts.\nTwo of my personal favourites are,\n Sanskrit 2003 Noto Serif Devanagari  Code The source code for the entire setup is available at hrishikeshrt/devanagari-transliteration-latex.\n","date":1646737200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646737200,"objectID":"5bc9c800d251713fc16de65cddd76ac4","permalink":"/post/devanagari-transliteration-in-latex/","publishdate":"2022-03-08T16:30:00+05:30","relpermalink":"/post/devanagari-transliteration-in-latex/","section":"post","summary":"Devanagari text can be transliterated in several standard schemes. More often than not, a user has a preference of scheme to type the input in. Similarly, at times, one faces a need to render it in a different scheme in the PDF document. Here's a simple solution!","tags":["transliteration","devanagari","latex"],"title":"Devanagari Transliteration in LaTeX","type":"post"},{"authors":["Hrishikesh Terdalkar"],"categories":["software"],"content":"PyCDSL is a python interface to Cologne Digital Sanskrit Lexicon (CDSL).\n Free software: GNU General Public License v3 Documentation: https://pycdsl.readthedocs.io.  Features  CDSL Corpus Management (Download, Update, Access) Unified interface to access all dictionaries available at CDSL Console command and REPL interface for easy dictionary search Extensive support for transliteration using indic-transliteration module  Usage PyCDSL can be used in a python project, as a console command and as an interactive REPL interface.\n","date":1643308200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643308200,"objectID":"3c5cfab03c7c256a9bc9532d6500302a","permalink":"/project/pycdsl/","publishdate":"2022-01-28T00:00:00+05:30","relpermalink":"/project/pycdsl/","section":"project","summary":"PyCDSL is a python interface to Cologne Digital Sanskrit Lexicon (CDSL). It provides an unified method to download, update and programmatically access all the dictionaries available at CDSL. It also features a console command and a REPL interface to make dictionary search easy for non-programmers. PyCDSL has extensive support for transliteration using `indic-transliteration` module.","tags":["sanskrit","software","python","lexicon"],"title":"PyCDSL","type":"project"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":"","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"cb39e2ebbfca2a9f56c4f54311d1aa24","permalink":"/publication/fse2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/fse2021/","section":"publication","summary":"We present a web-based tool *Sangrahaka* for annotating entities and relationships from text corpora towards construction of a knowledge graph and subsequent querying using templatized natural language questions.  The application is language and corpus agnostic, but can be tuned for specific needs of a language or a corpus.  The application is freely available for download and installation.  Besides having a user-friendly interface, it is fast, supports customization, and is fault tolerant on both client and server side.  It outperforms other annotation tools in an objective evaluation metric.  The framework has been successfully used in two annotation tasks.","tags":["software","annotation","querying","knowledge graph"],"title":"Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs","type":"publication"},{"authors":["Hrishikesh Terdalkar"],"categories":["demo"],"content":"There are virtually unlimited options available for building a personal homepage. One can find several options based on the purpose, aesthetics, technologies and difficulty.\nSome of the easiest options include Google Sites, WordPress etc. If one wants to have a bit more control and customizability, there are static site builders like Hugo, Jekyll, Gatsby, etc. which can be hosted on GitHub Pages, Netlify etc. For an academic portfolio, I found \u0026ldquo;Hugo Academic\u0026rdquo; to be perfect, both aesthetically and feature-wise.\n\u0026ldquo;Hugo Academic\u0026rdquo; comes with an easy deployment option to Netlify, which doesn\u0026rsquo;t require the user to write any code. To deploy it to GitHub is not as straightforward and too many guides for too many versions might confuse a novice user. Here, I am listing down the steps I followed to setup this website.\nGet Started using GitHub Template  Install Hugo Go to \u0026ldquo;Hugo Academic\u0026rdquo; Starter Template Click on Use this template to create a repository www (you may choose a different name) Clone your repository locally git clone git@github.com:\u0026lt;your-username\u0026gt;/www.git www cd www git submodule update --init --recursive Create an empty repository \u0026lt;your-username\u0026gt;.github.io on GitHub, if you haven\u0026rsquo;t already.  Note: If you do not have GitHub premium, this must be a public repository in order to use GitHub Pages Set-up the GitHub Pages from repository settings.   Add the website repository as a submodule, git submodule add -f -b master git@github.com:\u0026lt;your-username\u0026gt;/\u0026lt;your-username\u0026gt;.github.io.git public Edit content in the content directory Test locally, hugo server Generate public pages, hugo. This will generate the static website in the public/ directory. Commit content to the submodule (website repository) first, cd public, git add . and git commit -m \u0026quot;Website\u0026quot; Commit content to the base repository, git add . and git commit -m \u0026quot;Content\u0026quot;  Reference Documentation: https://wowchemy.com/docs/\n Following are the contents of the sample post that come with the installation. I am keeping them here since they contain useful and detailed instructions.\nOriginal Instructions Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  Create a new site Personalize your site Chat with the Wowchemy community or Hugo community Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy Request a feature or report a bug for Wowchemy Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\nClick here to become a sponsor and help support Wowchemy\u0026rsquo;s future As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Setting up a personal academic portfolio using `Academic` theme for `Hugo` is really easy and results in an aesthetically pleasing website.","tags":["homepage","hugo","academic"],"title":"Setting up a personal academic homepage","type":"post"},{"authors":["Hrishikesh Terdalkar"],"categories":["software"],"content":" Free software: GNU General Public License v3 Documentation: https://google-drive-ocr.readthedocs.io.  Features  Perform OCR using Google\u0026rsquo;s Drive API v3 Class GoogleOCRApplication() for use in projects Highly configurable CLI Run OCR on a single image file Run OCR on multiple image files Run OCR on all images in directory Use multiple workers (multiprocessing) Work on a PDF document directly  Install To install Google OCR (Drive API v3), run this command in your terminal:\npip install google-drive-ocr  Note: One must setup a Google application and download client_secrets.json file before using google_drive_ocr.\nSetup  Create a project on Google Cloud Platform Wizard: https://console.developers.google.com/start/api?id=drive  Instructions  https://cloud.google.com/genomics/downloading-credentials-for-api-access Select application type as \u0026ldquo;Installed Application\u0026rdquo; Create credentials \u0026ldquo;OAuth consent screen\u0026rdquo; \u0026ndash;\u0026gt; \u0026ldquo;OAuth client ID\u0026rdquo; Save client_secret.json  Usage Using in a Project Create a GoogleOCRApplication application instance:\nfrom google_drive_ocr import GoogleOCRApplication app = GoogleOCRApplication('client_secret.json')  Perform OCR on a single image:\napp.perform_ocr('image.png')  Perform OCR on mupltiple images:\napp.perform_batch_ocr(['image_1.png', 'image_2.png', 'image_3.png'])  Perform OCR on multiple images using multiple workers (multiprocessing):\napp.perform_batch_ocr(['image_1.png', 'image_3.png', 'image_2.png'], workers=2)  Using Command Line Interface Typical usage with several options:\ngoogle-ocr --client-secret client_secret.json \\ --upload-folder-id \u0026lt;google-drive-folder-id\u0026gt; \\ --image-dir images/ --extension .jpg \\ --workers 4 --no-keep  Show help message with the full set of options:\ngoogle-ocr --help  Configuration The default location for configuration is ~/.gdo.cfg. If configuration is written to this location with a set of options, we don\u0026rsquo;t have to specify those options again on the subsequent runs.\nSave configuration and exit:\ngoogle-ocr --client-secret client_secret.json --write-config ~/.gdo.cfg  Read configuration from a custom location (if it was written to a custom location):\ngoogle-ocr --config ~/.my_config_file ..  Performing OCR Note: It is assumed that the client-secret option is saved in configuration file.\nSingle image file:\ngoogle-ocr -i image.png  Multiple image files:\ngoogle-ocr -b image_1.png image_2.png image_3.png  All image files from a directory with a specific extension:\ngoogle-ocr --image-dir images/ --extension .png  Multiple workers (multiprocessing):\ngoogle-ocr -b image_1.png image_2.png image_3.png --workers 2  PDF files:\ngoogle-ocr --pdf document.pdf --pages 1-3 5 7-10 13  ","date":1612895400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646245800,"objectID":"52b002973888d06b487098578101b613","permalink":"/post/perform-ocr-using-google-drive-api/","publishdate":"2021-02-10T00:00:00+05:30","relpermalink":"/post/perform-ocr-using-google-drive-api/","section":"post","summary":"Google's Drive API can be used to perform OCR on images from any language. `google-drive-ocr` is a python package that allows users to do this with utmost ease, right from the terminal.","tags":["google","ocr","python","software"],"title":"Google Drive OCR","type":"post"},{"authors":["Hrishikesh Terdalkar"],"categories":[],"content":"","date":1603596600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603596600,"objectID":"5b8752ee830d2b1172e1cc5ceafc65e6","permalink":"/project/jnanasangraha/","publishdate":"2020-10-25T09:00:00+05:30","relpermalink":"/project/jnanasangraha/","section":"project","summary":"Jñānasaṅgrahaḥ is a collection of several web-based computational applications related to the Sanskrit language. The aim is to highlight the features of Sanskrit language in a way that is approachable for an enthusiastic user, even if she has a limited Sanskrit background. It comprises of (1) Saṅkhyāpaddhatiḥ, a collection of three ancient Indian numeral systems, Kaṭapayādi Saṅkhyā, Āryabhaṭīya Saṅkhyā and Bhūtasaṅkhyā and (2) Varṇajñānam, an interface and a library of utility functions related to varṇa (phonetic unit of Sanskrit language) information and manipulation.","tags":["sanskrit","software","user interface"],"title":"Jñānasaṅgrahaḥ","type":"project"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":"","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"671d2f359b3294593071ba7b7e036e48","permalink":"/publication/iscls2019/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/publication/iscls2019/","section":"publication","summary":"Sanskrit (*Saṃskṛta*) enjoys one of the largest and most varied literature in the whole world. Extracting the knowledge from it, however, is a challenging task due to multiple reasons including complexity of the language and paucity of standard natural language processing tools. In this paper, we target the problem of building knowledge graphs for particular types of relationships from Saṃskṛta texts. We build a natural language question-answering system in Saṃskṛta that uses the knowledge graph to answer factoid questions. We design a framework for the overall system and implement two separate instances of the system on human relationships from Mahābhārata and Rāmāyaṇa, and one instance on synonymous relationships from Bhāvaprakāśa Nighaṇṭu, a technical text from Āyurveda. We show that about 50% of the factoid questions can be answered correctly by the system. More importantly, we analyse the shortcomings of the system in detail for each step, and discuss the possible ways forward.","tags":["sanskrit","question answering","knowledge graph"],"title":"Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"01496dd6aa227594109fc4b7a09176ae","permalink":"/publication/iscls2019demo/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/publication/iscls2019demo/","section":"publication","summary":"The *Kaṭapayādi* system of encoding numbers as words by replacing each digit by a character was developed in ancient India. We present a web-based system that for conversion to and from the *Kaṭapayādi* numbering scheme. It can both decode a word into its corresponding number, and can encode a number into word(s).","tags":["sanskrit","software","demonstration"],"title":"KaTaPaYadi System","type":"publication"},{"authors":null,"categories":null,"content":"Sanskrit is one of the most ancient and richest languages in the world. Its literature boasts of text spanning every facet of life and contains works on mathematics, arts, sciences, religion, philosophy, etc. Unfortunately, the large volume of such works and the relative lack of proficiency in the language have kept treasures in those text hidden from the common man. Unraveling information from these texts in a targeted and systematic manner can not only help in enhancing the knowledge systems but can also revive an interest in the language.\nSanskrit enjoys one of the largest and most varied literature in the whole world. Extracting the knowledge from it, however, is a challenging task due to multiple reasons including complexity of the language and paucity of standard natural language processing tools. Therefore, we explore the construction of knowledge graphs through the process of manual annotation of a Sanskrit text, Vālmīki Rāmāyaṇa. We have developed a comprehensive NLP annotation tool, Antarlekhaka, which is being used for this large scale annotation task.\n","date":1558569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558569600,"objectID":"e0fc40bcb17badb7ddde1cd17d80c629","permalink":"/project/iks-aicte-ramayana-question-answering/","publishdate":"2019-05-23T00:00:00Z","relpermalink":"/project/iks-aicte-ramayana-question-answering/","section":"project","summary":"Under the umbrella of Indian Knowledge Systems (IKS) division of All India Council for Technical Education (AICTE), Government of India, we target the problem of building general purpose knowledge graph from Sanskrit text of Vālmīki Rāmāyaṇa as well as building natural language question answering system on top of the constructed KG. Due to several limitations related to the state of the art of Sanskrit NLP for semantic tasks, we opt for the process of manual annotation. We have developed a comprehensive NLP annotation tool Antarlekhaka, which is being used for this large scale annotation task.","tags":["sanskrit","software","annotation","knowledge graph","question answering","natural language processing"],"title":"Question-Answering System for Ramayana","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" Hrishikesh is an Assistant Professor in the Department of Computer Science and Information Systems at BITS Pilani, Hyderabad Campus. Previously, he was a postdoctoral researcher at the LIRIS Research Lab, University of Lyon 1 hosted by Prof. Angela Bonifati and Prof. Andrea Mauri. He completed his PhD from the Department of Computer Science and Engineering, IIT Kanpur under the supervision of Prof. Arnab Bhattacharya.\nHis research lies in the intersection of Computational Linguistics, Natural Language Processing, and Graph Databases, with a particular focus on low-resource languages such as Sanskrit and other Indian languages. He is committed to leveraging the interdisciplinary background to pioneer innovations, with a strong emphasis on creating a real-world impact. His broader interests include Databases, Software, Information Retrieval, Artificial Intelligence, Human-Computer Interaction, and Data Mining.\nHe is looking for motivated students interested in pursuing research in the aforementioned research areas. If you are interested, please feel free to reach out.\n[ Curriculum Vitae ]\nNote to Students @ BITS Pilani Hyderabad Campus If you are interested in pursuing a project with me, please fill the Application Form\n","date":1753574400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1753574400,"objectID":"9e370bd7161e19b2ff5ea464c359c815","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hrishikesh is an Assistant Professor in the Department of Computer Science and Information Systems at BITS Pilani, Hyderabad Campus. Previously, he was a postdoctoral researcher at the LIRIS Research Lab, University of Lyon 1 hosted by Prof.","tags":null,"title":"Hrishikesh Terdalkar","type":"authors"},{"authors":["V S D S Mahesh Akavarapu","Hrishikesh Terdalkar","Pramit Bhattacharyya","Shubhangi Agarwal","Vishakha Deulgaonkar","Chaitali Dangarikar","Pralay Manna","Arnab Bhattacharya"],"categories":null,"content":" ","date":1753574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753574400,"objectID":"511a92c31bb019b9ccf15bd7fef1516d","permalink":"/publication/acl2025/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/acl2025/","section":"publication","summary":"Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages---Sanskrit, Ancient Greek and Latin---to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question–answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.","tags":["large language models","cross-lingual","zero-shot generalization","classical languages","sanskrit"],"title":"A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs","type":"publication"},{"authors":["Hrishikesh Terdalkar","Angela Bonifati","Andrea Mauri"],"categories":null,"content":" ","date":1750982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750982400,"objectID":"adbad764cfda2c2af8ffc638719e0e81","permalink":"/publication/gradesnda2025/","publishdate":"2025-05-01T00:00:00Z","relpermalink":"/publication/gradesnda2025/","section":"publication","summary":"Property graphs are widely used in domains such as healthcare, finance, and social networks, but they often contain errors due to inconsistencies, missing data, or schema violations. Traditional rule-based and heuristic-driven graph repair methods are limited in their adaptability as they need to be tailored for each dataset. On the other hand, interactive human-in-the-loop approaches may become infeasible when dealing with large graphs, as the cost--both in terms of time and effort--of involving users becomes too high. Recent advancements in Large Language Models (LLMs) present new opportunities for automated graph repair by leveraging contextual reasoning and their access to real-world knowledge. We evaluate the effectiveness of six open-source LLMs in repairing property graphs. We assess repair quality, computational cost, and model-specific performance. Our experiments show that LLMs have the potential to detect and correct errors, with varying degrees of accuracy and efficiency. We discuss the strengths, limitations, and challenges of LLM-driven graph repair and outline future research directions for improving scalability and interpretability.","tags":["large language models","property graphs","graph repair"],"title":"Graph Repairs with Large Language Models: An Empirical Study","type":"publication"},{"authors":["Mithilesh K","Amarjit Madhumalararungeethayan","Dharanish Rahul S","Abhijith Balan","C Oswald","Hrishikesh Terdalkar"],"categories":null,"content":" ","date":1733356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733356800,"objectID":"6ec39c7ae4e01033f525427b3cb9b46d","permalink":"/publication/paclic2024/","publishdate":"2024-10-22T00:00:00Z","relpermalink":"/publication/paclic2024/","section":"publication","summary":"In this work, we introduce a novel Grammar Question-Answering System (Aganittyam) and its associated corpus on the dravidian language Tamil. It is one of the oldest surviving languages, with a documented history spanning over 2,000 years. Tamil is a classical language and it is official in three countries including India and various diasporic communities around the world speak it. Learning Tamil Grammar is still challenging due to its Agglutination and Complex Morphology. We created a Tamil Grammar Corpus focusing all kinds of learners and manually annotated the corpus since automated tools are not efficient enough. This made us to create a ontology on entity types and relationship types for the same. We identified entities and relationships and store the resultant triplets (subject-predicate–object) as a Knowledge Graph (KG) consisting of 63,587 entities. We also developed a framework for templatized Question-Answering along with it. We performed bi-fold evaluation (Query metrics and Human-Centric based) with thorough experimentation and show that our QA system is robust, reliable and fun in answering various objective questions.","tags":["tamil","question answering","natural language processing","annotation","dataset","ontology"],"title":"Aganittyam: Learning Tamil Grammar through Knowledge Graph based Templatized Question Answering","type":"publication"},{"authors":["Chaitali Dangarikar","Arnab Bhattacharya","Karthika N J","Hrishikesh Terdalkar","Pramit Bhattacharyya","Annarao Kulkarni","Chaitanya S Lakkundi","Ganesh Ramakrishnan","Shivani V"],"categories":null,"content":" ","date":1729123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729123200,"objectID":"9efc45b9df4850b48f1fdfd8a9b5852b","permalink":"/publication/samanvaya/","publishdate":"2024-10-22T00:00:00Z","relpermalink":"/publication/samanvaya/","section":"publication","summary":"Interlingua serves as a constructed bridge language, designed to simplify communication and analysis across diverse natural languages by identifying commonalities in structure and meaning. Samanvaya, our proposed interlingua for Indian languages, embodies this principle by unifying linguistic features shared across these languages, facilitating deeper linguistic analysis and cross-lingual understanding while preserving their unique cultural identities.","tags":["sanskrit","interlingua","translation","annotation","natural language processing"],"title":"Samanvaya: An Interlingua for Unity of Indian Languages","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":" ","date":1701820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701820800,"objectID":"4240b3c7d84e2e4517818a4887dc2170","permalink":"/publication/nlposs2023/","publishdate":"2023-09-20T00:00:00Z","relpermalink":"/publication/nlposs2023/","section":"publication","summary":"One of the primary obstacles in the advancement of Natural Language Processing (NLP) technologies for low-resource languages is the lack of annotated datasets for training and testing machine learning models. In this paper, we present *Antarlekhaka*, a tool for manual annotation of a comprehensive set of tasks relevant to NLP. The tool is Unicode-compatible, language-agnostic, Web-deployable and supports distributed annotation by multiple simultaneous annotators. The system sports user-friendly interfaces for 8 categories of annotation tasks. These, in turn, enable the annotation of a considerably larger set of NLP tasks. The task categories include two linguistic tasks not handled by any other tool, namely, sentence boundary detection and deciding canonical word order, which are important tasks for text that is in the form of poetry. We propose the idea of sequential annotation based on small text units, where an annotator performs several tasks related to a single text unit before proceeding to the next unit. The research applications of the proposed mode of multi-task annotation are also discussed. *Antarlekhaka* outperforms other annotation tools in objective evaluation. It has been also used for two real-life annotation tasks on two different languages, namely, Sanskrit and Bengali. The tool is available at https://github.com/Antarlekhaka/code.","tags":["software","annotation","natural language processing"],"title":"Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language Annotation","type":"publication"},{"authors":["Hrishikesh Terdalkar","Vishakha Deulgaonkar","Arnab Bhattacharya"],"categories":null,"content":" ","date":1692921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692921600,"objectID":"df2fd85dce586fe926d578604021b9a6","permalink":"/publication/nyciks2023/","publishdate":"2023-08-25T00:00:00Z","relpermalink":"/publication/nyciks2023/","section":"publication","summary":"The Bṛhat-Trayī, consisting of Carakasaṃhitā, Suśrutasaṃhitā, and Aṣṭāṅgahṛdaya, is an encyclopaedic reference set in Āyurveda. However, the need for simpler texts led to the emergence of the Laghu-Trayī that includes Mādhavanidāna, Śārṅgadharasaṃhitā, and Bhāvaprakāśa. Authored by Ācārya Bhāvamiśra in the 16th century CE, Bhāvaprakāśa is a comprehensive work focused on medicine. The classification system of varga in its nighaṇṭu section, Bhāvaprakāśanighaṇṭu, categorizes substances based on type, origin, and medicinal properties. This valuable resource assists practitioners and researchers in Āyurveda. We present this information in an accessible manner to promote wider utilization of this knowledge. We create a robust ontology to capture the semantic information of medicinal substances, designing user-friendly interfaces for efficient annotation and curation, perform meticulous manual annotation on Bhāvaprakāśanighaṇṭu, and construct an accurate knowledge graph from three chapters of Bhāvaprakāśanighaṇṭu. The system is accessible at https://sanskrit.iitk.ac.in/ayurveda/.","tags":["sanskrit","ayurveda","annotation","querying","knowledge graph"],"title":"Āyurjñānam: Exploring Āyurveda using Knowledge Graphs","type":"publication"},{"authors":["Hrishikesh Terdalkar"],"categories":null,"content":" A Knowledge Base (KB) is a representation of real-world knowledge in a particular domain in a computer system. Knowledge Graphs (KG) are KBs that use graph as the underlying data structure. Knowledge-based systems are software that utilize knowledge bases to solve problems. In the field of Natural Language Processing (NLP), Question Answering (QA) is a problem of finding answers to natural language questions posed by humans. KGs are an integral part of addressing the problem of question answering. Difficulty of constructing knowledge graphs depends greatly on the language and the resources available, such as datasets, tools and technologies.\nSanskrit is a classical language with a vast amount of written literature on a wide variety of topics. However, most of this literature is not available in a format that is readily usable by computer systems. As a result, from a computational perspective, Sanskrit is still considered a low-resource language.\nIn this thesis, we make contributions towards the ultimate goal of question answering in Sanskrit through construction of various knowledge-based systems in Sanskrit. We first present a framework that attempts to answer factual questions through an automated construction of KGs. We highlight the shortcomings and limitations of the state-of-the-art of Sanskrit NLP. The scarcity of appropriate datasets poses a significant challenge in the development and evaluation of automated systems for knowledge graph construction. Human annotation plays an important role for the creation of such datasets. There is also a need for annotation tools with task-specific and intuitive interfaces to simplify the tedious task of manual annotation.\nWe present Sangrahaka, an annotator-friendly, web-based tool for ontology-driven annotation of entities and relationships towards the construction of knowledge graphs. It also supports querying. The tool is language and corpus-agnostic but customizable for specific needs. We demonstrate the usefulness of the tool through a real-world annotation task on Bhāvaprakāśanighaṇṭu, an Āyurveda text. We showcase a carefully constructed extensive ontology suitable for this task, resulting in annotations that contribute to the development of a knowledge graph and querying framework. These contributions are based on three chapters from Bhāvaprakāśanighaṇṭu.\nThen, we present Antarlekhaka, a general purpose multi-task annotation system for manual annotation of a comprehensive set of NLP tasks. The system supports annotation towards multiple categories of NLP tasks: sentence boundary detection, canonical word ordering, token annotation, token classification, token graph, sentence classification and sentence graph. The annotation is performed in a sequential manner for small logical units of text (e.g., a verse). We highlight the utility of the tool through the application of the tool for the annotation of Vālmīki Rāmāyaṇa, resulting in datasets for NLP tasks of sentence boundary detection, canonical word ordering, named entity recognition, action graph construction and co-reference resolution.\nBoth Sangrahaka and Antarlekhaka are presented as full-stack web-based software to support distributed annotation. They are designed to be easily configurable, web-deployable, customizable and with a multi-tier permission system. They are actively being used in real-world annotation tasks. The annotator-friendly and intuitive annotation interfaces of these tools have received positive feedback from the users, and they outperform other annotation tools in objective evaluation.\nSanskrit text corpora have undergone large-scale digitization efforts using OCR technology, inadvertently leading to the introduction of various errors. In this thesis, we present Chandojñānam, a system for identifying and utilizing Sanskrit meters. Apart from its core functionality of meter identification, the system also enables finding fuzzy matches based on sequence matching, thereby facilitating the correction of inaccuracies in digital corpora. The user-friendly interface of Chandojñānam displays the scansion, a graphical representation of the metrical pattern. Additionally, the system supports meter identification from uploaded images through the utilization of optical character recognition (OCR) engines. The text can be processed in either line-by-line mode or verse-by-verse mode.\nFinally, as part of our research contribution, we offer an extensive range of web-interfaces, tools, and software libraries specifically designed to highlight and utilize the computational aspects of Sanskrit. This diverse compilation includes Jñānasaṅgrahaḥ, a comprehensive web-based collection of various computational applications dedicated to the Sanskrit language. The overarching aim of Jñānasaṅgrahaḥ is to present the features of the Sanskrit language in an accessible manner, even for enthusiastic users with limited Sanskrit backgrounds. Within this collection, you will find Saṅkhyāpaddhatiḥ, a web-interface …","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"6aad632eb6eb4d81dc799d4ff1bf17a6","permalink":"/publication/phd/","publishdate":"2024-01-25T00:00:00Z","relpermalink":"/publication/phd/","section":"publication","summary":"We address the challenges and opportunities in the development of knowledge systems for Sanskrit, with a focus on question answering. By proposing a framework for the automated construction of knowledge graphs, introducing annotation tools for ontology-driven and general-purpose tasks, and offering a diverse collection of web-interfaces, tools, and software libraries, we have made significant contributions to the field of computational Sanskrit. These contributions not only enhance the accessibility and accuracy of Sanskrit text analysis but also pave the way for further advancements in knowledge representation and language processing. Ultimately, this research contributes to the preservation, understanding, and utilization of the rich linguistic information embodied in Sanskrit texts.","tags":["sanskrit","knowledge base","knowledge graph","annotation","querying","question answering","tools","software","computational linguistics","natural language processing","ayurveda"],"title":"Sanskrit Knowledge-based Systems: Annotation and Computational Tools","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":" ","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"965dec720687c23fd364e91333863e74","permalink":"/publication/wsc2023_2/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/publication/wsc2023_2/","section":"publication","summary":"We present Chandojñānam, a web-based Sanskrit meter (Chanda) identification and utilization system. In addition to the core functionality of identifying meters, it sports a friendly user interface to display the scansion, which is a graphical representation of the metrical pattern. The system supports identification of meters from uploaded images by using optical character recognition (OCR) engines in the backend. It is also able to process entire text files at a time. The text can be processed in two modes, either by treating it as a list of individual lines, or as a collection of verses. When a line or a verse does not correspond exactly to a known meter, Chandojñānam is capable of finding fuzzy (i.e., approximate and close) matches based on sequence matching. This opens up the scope of a meter based correction of erroneous digital corpora. The system is available for use at https://sanskrit.iitk.ac.in/jnanasangraha/chanda/, and the source code in the form of a Python library is made available at https://github.com/hrishikeshrt/chanda/.","tags":["sanskrit","prosody","meter identification","software"],"title":"Chandojñānam: A Sanskrit Meter Identification and Utilization System","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"22daa85821ec7dcf167ee65026ea63b8","permalink":"/publication/wsc2023demo_1/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_1/","section":"publication","summary":"Jñānasaṅgrahaḥ is a web-based collection of several computational applications related to the Sanskrit language. The aim is to highlight the features of Sanskrit language in a way that is approachable for an enthusiastic user, even if she has a limited Sanskrit background.","tags":["sanskrit","software","jnanasangraha","demonstration"],"title":"Jñānasaṅgrahaḥ: A Collection of Computational Applications related to Sanskrit","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"830f86ad6ed0cdfdf19cf8f655f85234","permalink":"/publication/wsc2023demo_3/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_3/","section":"publication","summary":"PyCDSL is a Python library that provides programmer friendly interface to Cologne Digital Sanskrit Dictionaries (CDSD). The library serves as a corpus management tool to download, update and access dictionaries from CDSD. The tool provides a command line interface for ease of search and a programmable interface for using CDSD in computational linguistic projects written in Python 3.","tags":["sanskrit","software","python","dictionary"],"title":"PyCDSL: A Programmatic Interface to Cologne Digital Sanskrit Dictionaries","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya","Madhulika Dubey","Ramamurthy S","Bhavna Naneria Singh"],"categories":null,"content":" ","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"551adc5ffa8aac4dce57ee070323c60a","permalink":"/publication/wsc2023_1/","publishdate":"2022-01-12T00:00:00Z","relpermalink":"/publication/wsc2023_1/","section":"publication","summary":"Knowledge bases (KB) are an important resource in a number of natural language processing (NLP) and information retrieval (IR) tasks, such as semantic search, automated question-answering etc. They are also useful for researchers trying to gain information from a text. Unfortunately, however, the state-of-the-art in Sanskrit NLP does not yet allow automated construction of knowledge bases due to unavailability or lack of sufficient accuracy of tools and methods. Thus, in this work, we describe our efforts on manual annotation of Sanskrit text for the purpose of knowledge graph (KG) creation. We choose the chapter Dhānyavarga from Bhāvaprakāśanighaṇṭu of the Ayurvedic text Bhāvaprakāśa for annotation. The constructed knowledge graph contains 410 entities and 764 relationships. Since Bhāvaprakāśanighaṇṭu is a technical glossary text that describes various properties of different substances, we develop an elaborate ontology to capture the semantics of the entity and relationship types present in the text. To query the knowledge graph, we design 31 query templates that cover most of the common question patterns. For both manual annotation and querying, we customize the Sangrahaka framework previously developed by us. The entire system including the dataset is available from https://sanskrit.iitk.ac.in/ayurveda. We hope that the knowledge graph that we have created through manual annotation and subsequent curation will help in development and testing of NLP tools in future as well as studying of the Bhāvaprakāśanighaṇṭu text.","tags":["sanskrit","ayurveda","annotation","knowledge graph","dataset","ontology"],"title":"Semantic Annotation and Querying Framework based on Semi-structured Ayurvedic Text","type":"publication"},{"authors":["Hrishikesh Terdalkar","V S D S Mahesh Akavarapu","Shubhangi Agarwal","Arnab Bhattacharya"],"categories":[],"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"773ed3c8591d86213296305109bfbb67","permalink":"/publication/wsc2023demo_2/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/wsc2023demo_2/","section":"publication","summary":"Vaiyyākaraṇaḥ is a Telegram bot aimed towards helping the learners of Sanskrit grammar (vyākaraṇa). The salient features of Vaiyyākaraṇaḥ are: stem finder (prātipadikam), declension generation (subantāḥ), root finder (dhātuḥ), conjugation generation (tiṅantāḥ) and word segmentation (sandhisamāsau). State-of-the-art datasets, tools and technologies are used to offer these capabilities.","tags":["sanskrit","software","grammar","telegram","chatbot"],"title":"Vaiyyākaraṇaḥ: A Sanskrit Grammar Bot for Telegram","type":"publication"},{"authors":["Jivnesh Sandhan","Ashish Gupta","Hrishikesh Terdalkar","Tushar Sandhan","Suvendu Samanta","Laxmidhar Behera","Pawan Goyal"],"categories":null,"content":" ","date":1665532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665532800,"objectID":"4db8b4ea56bc4195c89b37f929be7df4","permalink":"/publication/coling2022/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/publication/coling2022/","section":"publication","summary":"The phenomenon of compounding is ubiquitous in Sanskrit. It serves for achieving brevity in expressing thoughts, while simultaneously enriching the lexical and structural formation of the language. In this work, we focus on the Sanskrit Compound Type Identification (SaCTI) task, where we consider the problem of identifying semantic relations between the components of a compound word. Earlier approaches solely rely on the lexical information obtained from the components and ignore the most crucial contextual and syntactic information useful for SaCTI. However, the SaCTI task is challenging primarily due to the implicitly encoded context-sensitive semantic relation between the compound components. Thus, we propose a novel multi-task learning architecture which incorporates the contextual information and enriches the complementary syntactic information using morphological tagging and dependency parsing as two auxiliary tasks. Experiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and 7.7 points (F1-score) absolute gain compared to the state-of-the-art system. Further, our multi-lingual experiments demonstrate the efficacy of the proposed architecture in English and Marathi languages.","tags":["sanskrit","multi-task learning","compound identification","annotation"],"title":"A Novel Multi-Task Learning Approach for Context-Sensitive Compound Type Identification in Sanskrit","type":"publication"},{"authors":["Hrishikesh Terdalkar"],"categories":["demo","software"],"content":"Devanagari is the fourth most widely adopted writing system in the world, primarily used in the Indian subcontinent. The script is being used for more than 120 languages, some of the more notable languages being, Sanskrit, Hindi, Marathi, Pali, Nepali and several variations of these languages.\nDevanagari text can be transliterated in various standard schemes. There exist several input systems based on these transliteration schemes to enable users easily input the text. More often than not, a user has a preference of scheme to type the input in. Similarly, at times, one faces a need to render it in a different scheme in the PDF document.\nIn my case, I prefer using ibus-m17n to type text in Devanagari. While writing articles that contain Devanagari text, I also faced the need to render the text as IAST in the final PDF. One could always learn to input text in another input scheme, but that may get tedious. Similarly, transliterating each word using online systems such as Aksharamukha can also be a tedious task. So, I was looking for a way where I can type in Devanagari, and have it rendered in IAST after PDF compilation. As a solution, I came up with a system consisting of a small set of LaTeX commands to add custom syntax to LaTeX and a python transliteration script (based on indic-transliteration package) to serve as a middle-layer and process the LaTeX file to create a new LaTeX file with proper transliteration.\nLaTeX Compilation System with Transliteration Support There are two primary components to the system,\nLaTeX Synatx Transliteration Script LaTeX Syntax XeTeX (xelatex) and LuaTeX (lualatex) have good unicode support and can be used to write Devanagari text. In the current example, I mention the setup with XeTeX.\nWe first add the required packages in the preamble of the LaTeX (.tex) file.\n% This assumes your files are encoded as UTF8 \\usepackage[utf8]{inputenc} % Devanagari Related Packages \\usepackage{fontspec, xunicode, xltxtra} Using fontspec, we can define environments for font families, to write text in specific scripts. To write Devanagari text, one needs to have a Devanagari font available. (It is assumed here that one may need to write both in Devanagari as well as other transliteration schemes.)\nFor more on Devanagari fonts, you may check the fonts section of this document. In this section, it is assumed that Sanskrit 2003 font is installed in the system.\nTo define the environments as mentioned earlier, we add the following lines in the preamble.\n% Define Fonts \\newfontfamily\\textskt[Script=Devanagari]{Sanskrit 2003} \\newfontfamily\\textiast{Noto Serif} % Commands for Devanagari Transliterations \\newcommand{\\skt}[1]{{\\textskt{#1}}} \\newcommand{\\iast}[1]{{\\textiast{#1}}} \\newcommand{\\Iast}[1]{{\\textiast{#1}}} \\newcommand{\\IAST}[1]{{\\textiast{#1}}} This provides us with four commands. \\skt{} can be used to render Devanagari text. \\iast{}, \\Iast{} and \\IAST{} can be used to render devanagari text in IAST format in lower case, title case and upper case respectively. It should be noted that from the perspective of LaTeX engine, the commands \\iast{}, \\Iast{} and \\IAST{} are identical. They are just different syntactically to aid the python script to perform transliteration and apply appropriate modifications. It should further be noted that we can define new font families and new commands for any of the valid schemes as per the requirement, which can potentially give us additional commands such \\velthuis{}, \\hk{} and so on.\nMinimal Example Equipped with these commands, and some Devanagari text, we have a minimal example as follows, stored in the file minimal.tex,\n\\documentclass[10pt]{article} % This assumes your files are encoded as UTF8 \\usepackage[utf8]{inputenc} % Devanagari Related Packages \\usepackage{fontspec, xunicode, xltxtra} % Define Fonts \\newfontfamily\\textskt[Script=Devanagari]{Sanskrit 2003} \\newfontfamily\\textiast{Noto Serif} % Commands for Devanagari Transliterations \\newcommand{\\skt}[1]{{\\textskt{#1}}} \\newcommand{\\iast}[1]{{\\textiast{#1}}} \\newcommand{\\Iast}[1]{{\\textiast{#1}}} \\newcommand{\\IAST}[1]{{\\textiast{#1}}} \\title{Transliteration of Devanagari Text} \\author{Hrishikesh Terdalkar} \\begin{document} \\maketitle \\skt{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\iast{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\Iast{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\IAST{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\end{document} Transliteration Script The python script is used to perform transliteration and some clean-up on the LaTeX.\npython3 finalize.py minimal.tex final.tex This result in the content being transformed in the following way,\n% ... \\skt{को न्वस्मिन् साम्प्रतं लोके गुणवान् कश्च वीर्यवान्।} \\iast{ko nvasmin sāmprataṃ loke guṇavān kaśca vīryavān|} \\Iast{Ko Nvasmin Sāmprataṃ Loke Guṇavān Kaśca Vīryavān|} \\IAST{KO NVASMIN SĀMPRATAṂ LOKE GUṆAVĀN KAŚCA VĪRYAVĀN|} % ... We can now proceed to compile the final.tex file.\nxelatex final This …","date":1646737200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646737200,"objectID":"5bc9c800d251713fc16de65cddd76ac4","permalink":"/post/devanagari-transliteration-in-latex/","publishdate":"2022-03-08T16:30:00+05:30","relpermalink":"/post/devanagari-transliteration-in-latex/","section":"post","summary":"Devanagari text can be transliterated in several standard schemes. More often than not, a user has a preference of scheme to type the input in. Similarly, at times, one faces a need to render it in a different scheme in the PDF document. Here's a simple solution!","tags":["transliteration","devanagari","latex"],"title":"Devanagari Transliteration in LaTeX","type":"post"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":" ","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"cb39e2ebbfca2a9f56c4f54311d1aa24","permalink":"/publication/fse2021/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/fse2021/","section":"publication","summary":"We present a web-based tool *Sangrahaka* for annotating entities and relationships from text corpora towards construction of a knowledge graph and subsequent querying using templatized natural language questions.  The application is language and corpus agnostic, but can be tuned for specific needs of a language or a corpus.  The application is freely available for download and installation.  Besides having a user-friendly interface, it is fast, supports customization, and is fault tolerant on both client and server side.  It outperforms other annotation tools in an objective evaluation metric.  The framework has been successfully used in two annotation tasks.","tags":["software","annotation","querying","knowledge graph"],"title":"Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs","type":"publication"},{"authors":["Hrishikesh Terdalkar"],"categories":["demo"],"content":"There are virtually unlimited options available for building a personal homepage. One can find several options based on the purpose, aesthetics, technologies and difficulty.\nSome of the easiest options include Google Sites, WordPress etc. If one wants to have a bit more control and customizability, there are static site builders like Hugo, Jekyll, Gatsby, etc. which can be hosted on GitHub Pages, Netlify etc. For an academic portfolio, I found “Hugo Academic” to be perfect, both aesthetically and feature-wise.\n“Hugo Academic” comes with an easy deployment option to Netlify, which doesn’t require the user to write any code. To deploy it to GitHub is not as straightforward and too many guides for too many versions might confuse a novice user. Here, I am listing down the steps I followed to setup this website.\nGet Started using GitHub Template Install Hugo Go to “Hugo Academic” Starter Template Click on Use this template to create a repository www (you may choose a different name) Clone your repository locally git clone git@github.com:\u0026lt;your-username\u0026gt;/www.git www cd www git submodule update --init --recursive Create an empty repository \u0026lt;your-username\u0026gt;.github.io on GitHub, if you haven’t already. Note: If you do not have GitHub premium, this must be a public repository in order to use GitHub Pages Set-up the GitHub Pages from repository settings. Add the website repository as a submodule, git submodule add -f -b master git@github.com:\u0026lt;your-username\u0026gt;/\u0026lt;your-username\u0026gt;.github.io.git public Edit content in the content directory Test locally, hugo server Generate public pages, hugo. This will generate the static website in the public/ directory. Commit content to the submodule (website repository) first, cd public, git add . and git commit -m \u0026#34;Website\u0026#34; Commit content to the base repository, git add . and git commit -m \u0026#34;Content\u0026#34; Reference Documentation: https://wowchemy.com/docs/\nFollowing are the contents of the sample post that come with the installation. I am keeping them here since they contain useful and detailed instructions.\nOriginal Instructions Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started Create a new site Personalize your site Chat with the Wowchemy community or Hugo community Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy Request a feature or report a bug for Wowchemy Updating Wowchemy? View the Update Guide and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\nClick here to become a sponsor and help support Wowchemy’s future As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode …","date":1628467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628467200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2021-08-09T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Setting up a personal academic portfolio using `Academic` theme for `Hugo` is really easy and results in an aesthetically pleasing website.","tags":["homepage","hugo","academic"],"title":"Setting up a personal academic homepage","type":"post"},{"authors":["Hrishikesh Terdalkar"],"categories":["software"],"content":" Free software: GNU General Public License v3 Documentation: https://google-drive-ocr.readthedocs.io. Features Perform OCR using Google’s Drive API v3 Class GoogleOCRApplication() for use in projects Highly configurable CLI Run OCR on a single image file Run OCR on multiple image files Run OCR on all images in directory Use multiple workers (multiprocessing) Work on a PDF document directly Install To install Google OCR (Drive API v3), run this command in your terminal:\npip install google-drive-ocr Note: One must setup a Google application and download client_secrets.json file before using google_drive_ocr.\nSetup Create a project on Google Cloud Platform Wizard: https://console.developers.google.com/start/api?id=drive Instructions https://cloud.google.com/genomics/downloading-credentials-for-api-access Select application type as “Installed Application” Create credentials “OAuth consent screen” –\u0026gt; “OAuth client ID” Save client_secret.json Usage Using in a Project Create a GoogleOCRApplication application instance:\nfrom google_drive_ocr import GoogleOCRApplication app = GoogleOCRApplication(\u0026#39;client_secret.json\u0026#39;) Perform OCR on a single image:\napp.perform_ocr(\u0026#39;image.png\u0026#39;) Perform OCR on mupltiple images:\napp.perform_batch_ocr([\u0026#39;image_1.png\u0026#39;, \u0026#39;image_2.png\u0026#39;, \u0026#39;image_3.png\u0026#39;]) Perform OCR on multiple images using multiple workers (multiprocessing):\napp.perform_batch_ocr([\u0026#39;image_1.png\u0026#39;, \u0026#39;image_3.png\u0026#39;, \u0026#39;image_2.png\u0026#39;], workers=2) Using Command Line Interface Typical usage with several options:\ngoogle-ocr --client-secret client_secret.json \\ --upload-folder-id \u0026lt;google-drive-folder-id\u0026gt; \\ --image-dir images/ --extension .jpg \\ --workers 4 --no-keep Show help message with the full set of options:\ngoogle-ocr --help Configuration The default location for configuration is ~/.gdo.cfg. If configuration is written to this location with a set of options, we don’t have to specify those options again on the subsequent runs.\nSave configuration and exit:\ngoogle-ocr --client-secret client_secret.json --write-config ~/.gdo.cfg Read configuration from a custom location (if it was written to a custom location):\ngoogle-ocr --config ~/.my_config_file .. Performing OCR Note: It is assumed that the client-secret option is saved in configuration file.\nSingle image file:\ngoogle-ocr -i image.png Multiple image files:\ngoogle-ocr -b image_1.png image_2.png image_3.png All image files from a directory with a specific extension:\ngoogle-ocr --image-dir images/ --extension .png Multiple workers (multiprocessing):\ngoogle-ocr -b image_1.png image_2.png image_3.png --workers 2 PDF files:\ngoogle-ocr --pdf document.pdf --pages 1-3 5 7-10 13 ","date":1612895400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646245800,"objectID":"52b002973888d06b487098578101b613","permalink":"/post/perform-ocr-using-google-drive-api/","publishdate":"2021-02-10T00:00:00+05:30","relpermalink":"/post/perform-ocr-using-google-drive-api/","section":"post","summary":"Google's Drive API can be used to perform OCR on images from any language. `google-drive-ocr` is a python package that allows users to do this with utmost ease, right from the terminal.","tags":["google","ocr","python","software"],"title":"Google Drive OCR","type":"post"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":null,"content":" ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"671d2f359b3294593071ba7b7e036e48","permalink":"/publication/iscls2019/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/publication/iscls2019/","section":"publication","summary":"Sanskrit (*Saṃskṛta*) enjoys one of the largest and most varied literature in the whole world. Extracting the knowledge from it, however, is a challenging task due to multiple reasons including complexity of the language and paucity of standard natural language processing tools. In this paper, we target the problem of building knowledge graphs for particular types of relationships from Saṃskṛta texts. We build a natural language question-answering system in Saṃskṛta that uses the knowledge graph to answer factoid questions. We design a framework for the overall system and implement two separate instances of the system on human relationships from Mahābhārata and Rāmāyaṇa, and one instance on synonymous relationships from Bhāvaprakāśa Nighaṇṭu, a technical text from Āyurveda. We show that about 50% of the factoid questions can be answered correctly by the system. More importantly, we analyse the shortcomings of the system in detail for each step, and discuss the possible ways forward.","tags":["sanskrit","question answering","knowledge graph"],"title":"Framework for Question-Answering in Sanskrit through Automated Construction of Knowledge Graphs","type":"publication"},{"authors":["Hrishikesh Terdalkar","Arnab Bhattacharya"],"categories":[],"content":"","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"01496dd6aa227594109fc4b7a09176ae","permalink":"/publication/iscls2019demo/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/publication/iscls2019demo/","section":"publication","summary":"The *Kaṭapayādi* system of encoding numbers as words by replacing each digit by a character was developed in ancient India. We present a web-based system that for conversion to and from the *Kaṭapayādi* numbering scheme. It can both decode a word into its corresponding number, and can encode a number into word(s).","tags":["sanskrit","software","demonstration"],"title":"KaTaPaYadi System","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d60c6104584881f09e843b48aa0ccbc2","permalink":"/administrative/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/administrative/","section":"","summary":"","tags":null,"title":"","type":"landing"}]